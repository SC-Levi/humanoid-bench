defaults:
    - override hydra/launcher: submitit_local

# environment
task: humanoid_h1hand-truck-v0
obs: state
episodic: false

# evaluation
checkpoint: ???
eval_episodes: 10
eval_freq: 50000

# training
trainer: director
steps: 3_000_000    

batch_size: 256
reward_coef: 0.1
value_coef: 0.1
termination_coef: 1
consistency_coef: 20
rho: 0.5
lr: 3e-4
enc_lr_scale: 0.3
grad_clip_norm: 20
tau: 0.01
discount_denom: 5
discount_min: 0.95
discount_max: 0.995
buffer_size: 1_000_000
exp_name: default
data_dir: '/media/levi/Singe4linux/Moore-TDMPC/TDMPC2/data/mt30'

# director hierarchy config
manager:
  goal_interval: 10          # Manager acts every 10 steps
  hidden_dim: 512           # Manager network hidden dimension
  ent_coef: 0.01           # Entropy coefficient for exploration
  lr: 3e-4                 # Manager learning rate
  gamma: 0.99              # Discount factor for GAE
  lam: 0.95                # Lambda for GAE computation
  tau: 0.005               # Soft update rate for target networks
  intrinsic_coef: 0.1      # Weight for intrinsic reward

goal_vae:
  n_latents: 8             # Number of latent variables
  n_classes: 8             # Number of classes per latent
  tau_start: 1.0           # Initial Gumbel-Sboftmax temperature
  tau_end: 0.1             # Final Gumbel-Softmax temperature
  anneal_steps: 100000     # Steps for temperature annealing
  beta: 1.0                # Beta weight for VAE loss
  lr: 3e-4                 # Goal-VAE learning rate

# planning
mpc: true
iterations: 6
num_samples: 512
num_elites: 64
num_pi_trajs: 24
horizon: 6
min_std: 0.05
max_std: 2
temperature: 0.5

# actor
log_std_min: -10
log_std_max: 2
entropy_coef: 1e-4

# critic
num_bins: 101
vmin: -10
vmax: +10

#moore
use_moe: true
n_experts: 8
use_orthogonal : true

# MoE annealing parameters
moe_tau_init: 1.8          # Initial temperature for Gumbel-Softmax
moe_tau_min: 0.5           # Minimum temperature (allows deeper specialization)
moe_tau_max: 2.0           # Maximum temperature
moe_beta: 0.02             # Temperature feedback rate (lower prevents rapid bottoming)
moe_lb_alpha: 3e-2         # Load balancing regularization weight (increased from 1e-2)
moe_freeze_frac: 0.05      # Fraction of training to freeze temperature (first 5%)

# architecture
model_size: 5
num_enc_layers: 2
enc_dim: 256
num_channels: 32
mlp_dim: 512
latent_dim: 512
task_dim: 96
num_q: 5
dropout: 0.01
simnorm_dim: 8

# logging
wandb_project: moore-tdmpc
wandb_entity: OA-MBRL
wandb_silent: false
disable_wandb: false
save_csv: true
visualize_experts: true    # whether to visualize expert gating weights
viz_interval: 10000         # how often to visualize (in steps/iterations)

# misc
compile: false
save_video: true
save_agent: true
seed: 0

# convenience
work_dir: ???
task_title: ???
multitask: ???
tasks: ???
obs_shape: ???
action_dim: ???
episode_length: ???
obs_shapes: ???
action_dims: ???
episode_lengths: ???
seed_steps: ???
bin_size: ???
